{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp \"/content/drive/MyDrive/kaggle.json\" ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d adelfr2009/ip102-new --unzip --force # download data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvyBcO5jgpzM",
        "outputId": "1ff1ace9-bb82-466d-f47a-4d11079762c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Dataset URL: https://www.kaggle.com/datasets/adelfr2009/ip102-new\n",
            "License(s): unknown\n",
            "Downloading ip102-new.zip to /content\n",
            " 98% 3.63G/3.70G [00:10<00:00, 528MB/s]\n",
            "100% 3.70G/3.70G [00:10<00:00, 364MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lYJgJ5JkAGi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttg-IXJESvem",
        "outputId": "5ef85432-4200-4615-b26b-519c8ade29a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing train set (45095 images)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying train: 100%|██████████| 45095/45095 [00:06<00:00, 7076.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing val set (7508 images)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying val: 100%|██████████| 7508/7508 [00:01<00:00, 7347.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing test set (22619 images)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying test: 100%|██████████| 22619/22619 [00:03<00:00, 7456.56it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths\n",
        "base_dir = \"/content/ip102/Classification\"\n",
        "images_dir = os.path.join(base_dir, \"images\")\n",
        "output_dir = \"/content/ip102dataset\"\n",
        "\n",
        "# Include test split\n",
        "splits = {\n",
        "    \"train\": os.path.join(base_dir, \"train.txt\"),\n",
        "    \"val\": os.path.join(base_dir, \"val.txt\"),\n",
        "    \"test\": os.path.join(base_dir, \"test.txt\"),\n",
        "}\n",
        "\n",
        "# Process each split\n",
        "for split, split_file in splits.items():\n",
        "    if not os.path.exists(split_file):\n",
        "        print(f\"Missing file: {split_file}\")\n",
        "        continue\n",
        "\n",
        "    with open(split_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    print(f\"Processing {split} set ({len(lines)} images)...\")\n",
        "\n",
        "    for line in tqdm(lines, desc=f\"Copying {split}\"):\n",
        "        img_name, class_id = line.strip().split()\n",
        "        src = os.path.join(images_dir, img_name)\n",
        "        dst_dir = os.path.join(output_dir, split, f\"class{class_id}\")\n",
        "        dst = os.path.join(dst_dir, img_name)\n",
        "\n",
        "        os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "        if os.path.exists(src):\n",
        "            shutil.copy2(src, dst)\n",
        "        else:\n",
        "            print(f\"Image not found: {src}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IovzVXyYLnP-"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import time\n",
        "import yaml\n",
        "import os\n",
        "import glob\n",
        "import math\n",
        "import logging\n",
        "from collections import OrderedDict\n",
        "from contextlib import suppress\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.utils\n",
        "from torch.nn.parallel import DistributedDataParallel as NativeDDP\n",
        "\n",
        "from timm.data import (\n",
        "    create_dataset,\n",
        "    create_loader,\n",
        "    resolve_data_config,\n",
        "    Mixup,\n",
        "    FastCollateMixup,\n",
        "    AugMixDataset,\n",
        ")\n",
        "from timm.models import (\n",
        "    create_model,\n",
        "    safe_model_name,\n",
        "    resume_checkpoint,\n",
        "    load_checkpoint,\n",
        "    model_parameters,\n",
        ")\n",
        "from timm.layers import convert_splitbn_model\n",
        "from timm.utils import *\n",
        "from timm.loss import *\n",
        "from timm.optim import create_optimizer_v2, optimizer_kwargs\n",
        "from timm.scheduler import create_scheduler\n",
        "from timm.utils import ApexScaler, NativeScaler\n",
        "\n",
        "#from misc.distillation_loss import DistillationLoss\n",
        "#from misc.cosine_annealing import CosineWDSchedule"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksdPAGTgpa4O",
        "outputId": "fc948ce2-9c20-4139-a933-a6df667a6767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.0)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from timm.models import create_model\n",
        "\n",
        "# Step 1: Create the same model architecture\n",
        "model = create_model(\n",
        "    \"fastvit_sa12\",\n",
        "    pretrained=False,\n",
        "    num_classes=102,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Load your trained checkpoint\n",
        "checkpoint = torch.load('/content/drive/MyDrive/fastvit_sa12.pth.tar')\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "#model_inf = reparameterize_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUF_DnbcfO3i",
        "outputId": "533982c4-dd93-4a23-c7f5-7e9035da6348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the model\n",
        "x = torch.randn(1, 3, 224, 224, requires_grad=True)\n",
        "\n",
        "torch.onnx.export(model,              # model being run\n",
        "                  x,                         # model input (or a tuple for multiple inputs)\n",
        "                  \"fastvit.onnx\", # where to save the model (can be a file or file-like object)\n",
        "                  export_params=True,        # store the trained parameter weights inside the model file\n",
        "                  opset_version=14,          # the ONNX version to export the model to\n",
        "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                  input_names = ['input'],   # the model's input names\n",
        "                  output_names = ['output']) # the model's output names"
      ],
      "metadata": {
        "id": "NkdFfLPer_gQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}